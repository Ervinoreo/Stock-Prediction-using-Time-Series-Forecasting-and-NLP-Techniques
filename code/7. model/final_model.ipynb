{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "nokQPsrNRVI6",
      "metadata": {
        "id": "nokQPsrNRVI6"
      },
      "source": [
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "276bd1c5-f64f-46cf-9d9a-eb2e3dee7b68",
      "metadata": {
        "id": "276bd1c5-f64f-46cf-9d9a-eb2e3dee7b68"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(r\"./Word_Embedding.csv\", skiprows=[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "xRJs9ZqRgp95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "xRJs9ZqRgp95",
        "outputId": "c990f8b4-d4f5-49ec-dabd-db681d7fdad3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dates</th>\n",
              "      <th>Closed Price</th>\n",
              "      <th>Combined_Desc</th>\n",
              "      <th>Percentage_Change</th>\n",
              "      <th>lemmatized_text</th>\n",
              "      <th>embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010-06-30</td>\n",
              "      <td>1.588667</td>\n",
              "      <td>tesla roadster reaches chinas great wall tesla...</td>\n",
              "      <td>-0.251148</td>\n",
              "      <td>tesla roadster reach china great wall tesla op...</td>\n",
              "      <td>[0.         0.         0.065174   0.08753883 0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Dates  Closed Price  \\\n",
              "0  2010-06-30      1.588667   \n",
              "\n",
              "                                       Combined_Desc  Percentage_Change  \\\n",
              "0  tesla roadster reaches chinas great wall tesla...          -0.251148   \n",
              "\n",
              "                                     lemmatized_text  \\\n",
              "0  tesla roadster reach china great wall tesla op...   \n",
              "\n",
              "                                          embeddings  \n",
              "0  [0.         0.         0.065174   0.08753883 0...  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "OsRIJe0VgzVa",
      "metadata": {
        "id": "OsRIJe0VgzVa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from ast import literal_eval\n",
        "\n",
        "# convert string representations into NumPy arrays\n",
        "def convert_str_to_array(embed_str):\n",
        "    embed_str = embed_str[1:-1]\n",
        "    str_nums = embed_str.strip().split()\n",
        "    return np.array([float(num) for num in str_nums if num], dtype=float)\n",
        "\n",
        "df['embeddings'] = df['embeddings'].apply(convert_str_to_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "kCMxDz_jkhmd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kCMxDz_jkhmd",
        "outputId": "b26d9aa6-c5b6-4662-b2e3-72be61254f9d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dates</th>\n",
              "      <th>Closed Price</th>\n",
              "      <th>Combined_Desc</th>\n",
              "      <th>Percentage_Change</th>\n",
              "      <th>lemmatized_text</th>\n",
              "      <th>embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010-06-30</td>\n",
              "      <td>1.588667</td>\n",
              "      <td>tesla roadster reaches chinas great wall tesla...</td>\n",
              "      <td>-0.251148</td>\n",
              "      <td>tesla roadster reach china great wall tesla op...</td>\n",
              "      <td>[0.0, 0.0, 0.065174, 0.08753883, 0.04256443, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-07-01</td>\n",
              "      <td>1.464000</td>\n",
              "      <td>tesla roadster gets version 25 upgrade tesla r...</td>\n",
              "      <td>-7.847274</td>\n",
              "      <td>tesla roadster get version 25 upgrade tesla re...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.06776106, 0.0, 0.0, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2010-07-02</td>\n",
              "      <td>1.280000</td>\n",
              "      <td>tesla premiers roadster 25 goodwood festival s...</td>\n",
              "      <td>-12.568307</td>\n",
              "      <td>tesla premier roadster 25 goodwood festival sp...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2010-07-06</td>\n",
              "      <td>1.074000</td>\n",
              "      <td>tesla motors faces rough road electriccar busi...</td>\n",
              "      <td>-16.093748</td>\n",
              "      <td>tesla motor face rough road electriccar busine...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07788439...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-07-07</td>\n",
              "      <td>1.053333</td>\n",
              "      <td>electriccar lovers already lining teslas model...</td>\n",
              "      <td>-1.924298</td>\n",
              "      <td>electriccar lover already lining tesla model s...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Dates  Closed Price  \\\n",
              "0  2010-06-30      1.588667   \n",
              "1  2010-07-01      1.464000   \n",
              "2  2010-07-02      1.280000   \n",
              "3  2010-07-06      1.074000   \n",
              "4  2010-07-07      1.053333   \n",
              "\n",
              "                                       Combined_Desc  Percentage_Change  \\\n",
              "0  tesla roadster reaches chinas great wall tesla...          -0.251148   \n",
              "1  tesla roadster gets version 25 upgrade tesla r...          -7.847274   \n",
              "2  tesla premiers roadster 25 goodwood festival s...         -12.568307   \n",
              "3  tesla motors faces rough road electriccar busi...         -16.093748   \n",
              "4  electriccar lovers already lining teslas model...          -1.924298   \n",
              "\n",
              "                                     lemmatized_text  \\\n",
              "0  tesla roadster reach china great wall tesla op...   \n",
              "1  tesla roadster get version 25 upgrade tesla re...   \n",
              "2  tesla premier roadster 25 goodwood festival sp...   \n",
              "3  tesla motor face rough road electriccar busine...   \n",
              "4  electriccar lover already lining tesla model s...   \n",
              "\n",
              "                                          embeddings  \n",
              "0  [0.0, 0.0, 0.065174, 0.08753883, 0.04256443, 0...  \n",
              "1  [0.0, 0.0, 0.0, 0.0, 0.06776106, 0.0, 0.0, 0.0...  \n",
              "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07788439...  \n",
              "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "NBE_1ZsPFa-Q",
      "metadata": {
        "id": "NBE_1ZsPFa-Q"
      },
      "outputs": [],
      "source": [
        "# time_steps is the number of previous day we are considering in a data point\n",
        "\n",
        "# include the percentage changes of previous \"time_steps\" days\n",
        "def create_combined_sequences(percentage_changes, embeddings, time_steps=3):\n",
        "    X_combined = []\n",
        "    y = []\n",
        "    for i in range(time_steps, len(percentage_changes)):\n",
        "        perc_change_sequence = percentage_changes[i-time_steps:i].flatten()\n",
        "        current_embedding = embeddings[i]\n",
        "        combined_features = np.concatenate([perc_change_sequence, current_embedding])\n",
        "        X_combined.append(combined_features)\n",
        "        y.append(percentage_changes[i])\n",
        "    return np.array(X_combined), np.array(y)\n",
        "\n",
        "percentage_changes = df['Percentage_Change'].fillna(0).values\n",
        "embeddings = df['embeddings']\n",
        "\n",
        "X, y = create_combined_sequences(percentage_changes, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4GbClMMOFvOn",
      "metadata": {
        "id": "4GbClMMOFvOn"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split the data into training and testing datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)\n",
        "\n",
        "# split the training dataset into training and validation datasets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dn6mZv9-0fh0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "dn6mZv9-0fh0",
        "outputId": "78c98186-965e-43b6-b064-c204b65345ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 1.72620372,  2.31126134, -1.34399379,  0.07858375,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.61719243,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.20736103,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.06964714,  0.        ,  0.08067048,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.1186376 ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.07253593,  0.        ,\n",
              "        0.07146797,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.05475319,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.12404214,  0.        ,  0.        ,  0.        ,\n",
              "        0.08392972,  0.        ,  0.        ,  0.        ,  0.07690957,\n",
              "        0.58292024,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.07534322,  0.        ,  0.38518328,\n",
              "        0.        ,  0.        ,  0.08178504,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.04717867,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "-0.8116079581635294"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(X_train[1])\n",
        "\n",
        "display(y_train[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z8f1Cr_CPaTq",
      "metadata": {
        "id": "Z8f1Cr_CPaTq"
      },
      "source": [
        "Base Model 1 - LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fvBGgXYl1FA",
      "metadata": {
        "id": "1fvBGgXYl1FA"
      },
      "source": [
        "kfold to determine params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "--64yRiLd-_P",
      "metadata": {
        "id": "--64yRiLd-_P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "# we are implementing forward pass in LSTM\n",
        "# units determines the dimensionality or size of the hidden state(number of neurons) in a LSTM layer.\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, units, dropout_rate, l2_lambda):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, units, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(units, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.dropout(x[:, -1, :])\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "aa3Ot_5MeIf3",
      "metadata": {
        "id": "aa3Ot_5MeIf3"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# add a new hyperparameter, named learning rate\n",
        "\n",
        "# The batch size specified in mini-batch SGD determines the number of samples processed in each mini-batch\n",
        "# Mini-batch SGD is used within each training fold.\n",
        "# In mini-batch SGD, the dataset is divided into small batches, and the model's parameters are updated based on the average gradient computed from each mini-batch\n",
        "# let say batch size is 5, in the k-fold cross validation, for the first iteration, imagine the first (k-1) fold is training dataset, the last fold is validation dataset, within each fold, the batch size will split each fold into 5, and update the parameters\n",
        "\n",
        "# This returns the average validation loss computed over all mini-batches in the validation dataset.\n",
        "def train_and_evaluate_model(X_train_fold, y_train_fold, X_val_fold, y_val_fold, units, dropout_rate, learning_rate, l2_lambda, optimizer_choice, epochs, batch_size):\n",
        "    train_dataset = torch.utils.data.TensorDataset(torch.Tensor(X_train_fold), torch.Tensor(y_train_fold))\n",
        "    val_dataset = torch.utils.data.TensorDataset(torch.Tensor(X_val_fold), torch.Tensor(y_val_fold))\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = LSTMModel(input_dim=X_train_fold.shape[2], units=units, dropout_rate=dropout_rate, l2_lambda=l2_lambda)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    if optimizer_choice == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)  # weight_decay is L2 regularization\n",
        "    elif optimizer_choice == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "    elif optimizer_choice == 'sgd':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "    elif optimizer_choice == 'adagrad':\n",
        "        optimizer = optim.Adagrad(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss = criterion(outputs, targets)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    return avg_val_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vcy0Zla7HQLP",
      "metadata": {
        "id": "vcy0Zla7HQLP"
      },
      "source": [
        "Bayesian Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "vHCXElK17sXe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHCXElK17sXe",
        "outputId": "d227eb9d-9fd2-4c58-b831-e5d6f70a9420"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "import hyperopt\n",
        "from hyperopt import hp, fmin, tpe, Trials\n",
        "import numpy as np\n",
        "\n",
        "units_options = np.arange(50, 325, 25).tolist()\n",
        "dropout_rate_options = np.arange(0.1, 0.35, 0.05).tolist()\n",
        "l2_lambda_options = np.arange(0.01, 0.1, 0.01).tolist()\n",
        "optimizer_options = ['sgd', 'adagrad', 'adam', 'rmsprop']\n",
        "learning_rate_options = np.arange(0.01, 0.1, 0.01).tolist()\n",
        "\n",
        "print(type(units_options))\n",
        "\n",
        "search_space = {\n",
        "    'units': hp.choice('units', units_options),\n",
        "    'learning_rate': hp.choice('learning_rate', learning_rate_options),\n",
        "    'dropout_rate': hp.choice('dropout_rate', dropout_rate_options),\n",
        "    'l2_lambda': hp.choice('l2_lambda', l2_lambda_options),\n",
        "    'optimizer_choice': hp.choice('optimizer_choice', optimizer_options)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "RC7OPK81-KSB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC7OPK81-KSB",
        "outputId": "8c4d3a48-2fec-463c-e4ec-a168ab0bf5cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 3/50 [00:35<09:24, 12.00s/trial, best loss: 13.03793507366252] "
          ]
        }
      ],
      "source": [
        "X = np.array(X_train)\n",
        "y = np.array(y_train).reshape(-1, 1)\n",
        "\n",
        "n_splits = 7\n",
        "epochs = 7\n",
        "batch_size = 16\n",
        "\n",
        "def objective(params):\n",
        "    units = int(params['units'])\n",
        "    learning_rate = params['learning_rate']\n",
        "    dropout_rate = params['dropout_rate']\n",
        "    l2_lambda = params['l2_lambda']\n",
        "    optimizer_choice = params['optimizer_choice']\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    scores = []\n",
        "    for train_index, val_index in kf.split(X):\n",
        "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
        "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
        "\n",
        "        X_train_fold = X_train_fold.reshape((X_train_fold.shape[0], 1, X_train_fold.shape[1]))\n",
        "        X_val_fold = X_val_fold.reshape((X_val_fold.shape[0], 1, X_val_fold.shape[1]))\n",
        "\n",
        "        score = train_and_evaluate_model(X_train_fold, y_train_fold, X_val_fold, y_val_fold, units, dropout_rate, learning_rate, l2_lambda, optimizer_choice, epochs, batch_size)\n",
        "        scores.append(score)\n",
        "\n",
        "    avg_score = np.mean(scores)\n",
        "    return avg_score\n",
        "\n",
        "# Perform Bayesian optimization\n",
        "trials = Trials()\n",
        "best = fmin(\n",
        "    fn=objective,\n",
        "    space=search_space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=50,  # Adjust the number of evaluations as needed\n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = {\n",
        "    'units': units_options[int(best['units'])],\n",
        "    'learning_rate': learning_rate_options[int(best['learning_rate'])],\n",
        "    'dropout_rate': dropout_rate_options[int(best['dropout_rate'])],\n",
        "    'l2_lambda': l2_lambda_options[int(best['l2_lambda'])],\n",
        "    'optimizer_choice': optimizer_options[int(best['optimizer_choice'])]\n",
        "}\n",
        "\n",
        "print(\"Best hyperparameters:\", best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XYxq43XeHWCB",
      "metadata": {
        "id": "XYxq43XeHWCB"
      },
      "source": [
        "Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bsm2lAqJedHM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "Bsm2lAqJedHM",
        "outputId": "83879e00-f1ff-40b3-88a3-733075128b1b"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "n_splits = 7\n",
        "epochs = 5\n",
        "batch_size = 16\n",
        "\n",
        "units_options = np.arange(50, 525, 25).tolist()\n",
        "dropout_rate_options = np.arange(0.1, 0.35, 0.05).tolist()\n",
        "l2_lambda_options = np.arange(0.01, 0.1, 0.01).tolist()\n",
        "optimizer_options = ['sgd', 'adagrad', 'adam', 'rmsprop']\n",
        "learning_rate_options = np.arange(0.01, 0.1, 0.01).tolist()\n",
        "\n",
        "# 把random_state 换去None\n",
        "# Hyperparameter : units, learning_rate, dropout_rate, l2_lambda, optimizer_choice\n",
        "\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=None)\n",
        "\n",
        "best_score = np.inf\n",
        "best_params = {}\n",
        "\n",
        "X = np.array(X_train)\n",
        "y = np.array(y_train).reshape(-1, 1)\n",
        "\n",
        "# Avoid Grid Search\n",
        "# Use Bayesian Hyperparameter Optimization\n",
        "for units in units_options:\n",
        "    for learning_rate in learning_rate_options:\n",
        "        for dropout_rate in dropout_rate_options:\n",
        "            for l2_lambda in l2_lambda_options:\n",
        "                for optimizer_choice in optimizer_options:\n",
        "                    scores = []\n",
        "                    for train_index, val_index in kf.split(X):\n",
        "                        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
        "                        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
        "\n",
        "                        X_train_fold = X_train_fold.reshape((X_train_fold.shape[0], 1, X_train_fold.shape[1]))\n",
        "                        X_val_fold = X_val_fold.reshape((X_val_fold.shape[0], 1, X_val_fold.shape[1]))\n",
        "\n",
        "                        score = train_and_evaluate_model(X_train_fold, y_train_fold, X_val_fold, y_val_fold, units, dropout_rate, learning_rate, l2_lambda, optimizer_choice, epochs, batch_size)\n",
        "                        scores.append(score)\n",
        "                        print(scores)\n",
        "\n",
        "                    avg_score = np.mean(scores)\n",
        "                    print(f\"Units: {units}, Learning rate: {learning_rate}, Dropout: {dropout_rate}, L2: {l2_lambda}, Optimizer: {optimizer_choice}, Avg Val Loss: {avg_score}\")\n",
        "\n",
        "                    if avg_score < best_score:\n",
        "                        best_score = avg_score\n",
        "                        best_params = {\n",
        "                            'units': units,\n",
        "                            'dropout_rate': dropout_rate,\n",
        "                            'l2_lambda': l2_lambda,\n",
        "                            'optimizer': optimizer_choice,\n",
        "                            'learning_rate': learning_rate\n",
        "                        }\n",
        "\n",
        "print(\"Best avg validation loss:\", best_score)\n",
        "print(\"Best hyperparameters:\", best_params)\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZYJmKTbcl43h",
      "metadata": {
        "id": "ZYJmKTbcl43h"
      },
      "source": [
        "building the model and test on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-vFViL85gh7v",
      "metadata": {
        "id": "-vFViL85gh7v"
      },
      "outputs": [],
      "source": [
        "class CustomLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, units, dropout_rate):\n",
        "        super(CustomLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=units,\n",
        "                            batch_first=True, dropout=dropout_rate)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(units, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, (hn, cn) = self.lstm(x)\n",
        "        out = self.dropout(lstm_out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "    \n",
        "input_dim = len(X_train[0])\n",
        "\n",
        "# the value here should retrieved from best_params(include best_optimizer)\n",
        "best_units = best_params.get('units')\n",
        "best_learning_rate = best_params.get('learning_rate')\n",
        "best_dropout_rate = best_params.get('dropout_rate')\n",
        "best_l2_lambda = best_params.get('l2_lambda')\n",
        "best_optimizer = best_params.get('optimizer_choice')\n",
        "\n",
        "model_lstm = CustomLSTM(input_dim=input_dim, units=best_units, dropout_rate=best_dropout_rate)\n",
        "model_lstm = model_lstm.to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def bestOptimizer(x, learning_rate, l2_lambda):\n",
        "    if x == 'adam':\n",
        "        return optim.Adam(model_lstm.parameters(), lr=learning_rate, weight_decay=l2_lambda)  # weight_decay is L2 regularization\n",
        "    elif x == 'rmsprop':\n",
        "        return optim.RMSprop(model_lstm.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "    elif x == 'sgd':\n",
        "        return optim.SGD(model_lstm.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "    elif x == 'adagrad':\n",
        "        return optim.Adagrad(model_lstm.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "\n",
        "optimizer = bestOptimizer(best_optimizer, best_learning_rate, best_l2_lambda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jukt81fXhz8F",
      "metadata": {
        "id": "Jukt81fXhz8F"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "X_train_train = np.array(X_train)\n",
        "X_test_val = np.array(X_val)\n",
        "y_train_train = np.array(y_train).reshape(-1, 1)\n",
        "y_test_val = np.array(y_val).reshape(-1, 1)\n",
        "\n",
        "X_train_train = X_train_train.reshape((X_train_train.shape[0], 1, X_train_train.shape[1]))\n",
        "X_test_val = X_test_val.reshape((X_test_val.shape[0], 1, X_test_val.shape[1]))\n",
        "\n",
        "train_dataset = TensorDataset(torch.Tensor(X_train_train), torch.Tensor(y_train_train))\n",
        "test_dataset = TensorDataset(torch.Tensor(X_test_val), torch.Tensor(y_test_val))\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K7PU1DAelOjy",
      "metadata": {
        "id": "K7PU1DAelOjy"
      },
      "outputs": [],
      "source": [
        "epochs = 50\n",
        "\n",
        "model_lstm.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_lstm(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
        "\n",
        "model_lstm.eval()\n",
        "test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model_lstm(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "print(f'Average Test Loss: {avg_test_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zpt7NgY1nd2O",
      "metadata": {
        "id": "zpt7NgY1nd2O"
      },
      "source": [
        "save to csv the result to be input for meta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iz5ggib8nESL",
      "metadata": {
        "id": "iz5ggib8nESL"
      },
      "outputs": [],
      "source": [
        "model_lstm.eval()\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model_lstm(inputs)\n",
        "\n",
        "        # Move predictions and actuals to CPU and convert them to numpy arrays\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "predictions = np.array(predictions).flatten()  # Flattening in case the outputs are multi-dimensional\n",
        "actuals = np.array(actuals).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FDU7epkKnMKc",
      "metadata": {
        "id": "FDU7epkKnMKc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({'Predictions': predictions, 'Actuals': actuals})\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(\"predictions_vs_actuals_lstm.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dBdbDtMZS-sa",
      "metadata": {
        "id": "dBdbDtMZS-sa"
      },
      "source": [
        "Base Model 2 - MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XTs-UHv1mC2q",
      "metadata": {
        "id": "XTs-UHv1mC2q"
      },
      "source": [
        "kfold to determine params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4JrbmVrRS-Np",
      "metadata": {
        "id": "4JrbmVrRS-Np"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n6Mb1fRNV1mx",
      "metadata": {
        "id": "n6Mb1fRNV1mx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "class MLPModel1(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_units, dropout_rate):\n",
        "        super(MLPModel1, self).__init__()\n",
        "        # we are using two hidden layers, fc1 and fc2\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_units)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(hidden_units, hidden_units)\n",
        "        self.output = nn.Linear(hidden_units, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.output(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lrDoe7r5WDv4",
      "metadata": {
        "id": "lrDoe7r5WDv4"
      },
      "outputs": [],
      "source": [
        "X_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "dataset = TensorDataset(X_tensor, y_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cxgWS3OtYCF2",
      "metadata": {
        "id": "cxgWS3OtYCF2"
      },
      "outputs": [],
      "source": [
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sEtUnfTcYDNO",
      "metadata": {
        "id": "sEtUnfTcYDNO"
      },
      "outputs": [],
      "source": [
        "print(X_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lvQRa3dpYFVV",
      "metadata": {
        "id": "lvQRa3dpYFVV"
      },
      "outputs": [],
      "source": [
        "k_folds = 5\n",
        "num_epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xr5zuYO9YZvC",
      "metadata": {
        "id": "xr5zuYO9YZvC"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train_and_evaluate_model(train_loader, val_loader, hidden_units, dropout_rate, learning_rate, l2_lambda, optimizer_choice, num_epochs):\n",
        "    model = MLPModel1(input_dim=X_tensor.shape[1], hidden_units=hidden_units, dropout_rate=dropout_rate)\n",
        "    model = model.to(device)  # Move model to the appropriate device\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    if optimizer_choice == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)  # weight_decay is L2 regularization\n",
        "    elif optimizer_choice == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "    elif optimizer_choice == 'sgd':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "    elif optimizer_choice == 'adagrad':\n",
        "        optimizer = optim.Adagrad(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18b37061",
      "metadata": {},
      "source": [
        "Bayesian Optimization Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8704f4e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import hyperopt\n",
        "from hyperopt import hp, fmin, tpe, Trials\n",
        "import numpy as np\n",
        "\n",
        "hidden_units_options = np.arange(50, 325, 25).tolist()\n",
        "dropout_rate_options = np.arange(0.1, 0.35, 0.05).tolist()\n",
        "l2_lambda_options = np.arange(0.01, 0.1, 0.01).tolist()\n",
        "optimizer_options = ['sgd', 'adagrad', 'adam', 'rmsprop']\n",
        "learning_rate_options = np.arange(0.01, 0.1, 0.01).tolist()\n",
        "\n",
        "search_space = {\n",
        "    'hidden_units': hp.choice('hidden_units', hidden_units_options),\n",
        "    'learning_rate': hp.choice('learning_rate', learning_rate_options),\n",
        "    'dropout_rate': hp.choice('dropout_rate', dropout_rate_options),\n",
        "    'l2_lambda': hp.choice('l2_lambda', l2_lambda_options),\n",
        "    'optimizer_choice': hp.choice('optimizer_choice', optimizer_options)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a1676cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "k_folds = 5\n",
        "num_epochs = 50\n",
        "\n",
        "\n",
        "\n",
        "def objective(params):\n",
        "    hidden_units = int(params['hidden_units'])\n",
        "    learning_rate = params['learning_rate']\n",
        "    dropout_rate = params['dropout_rate']\n",
        "    l2_lambda = params['l2_lambda']\n",
        "    optimizer_choice = params['optimizer_choice']\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    fold_scores = []\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kf.split(X_tensor)):\n",
        "        train_subsampler = Subset(dataset, train_ids)\n",
        "        val_subsampler = Subset(dataset, val_ids)\n",
        "\n",
        "        train_loader = DataLoader(train_subsampler, batch_size=32, shuffle=True)\n",
        "        val_loader = DataLoader(val_subsampler, batch_size=32, shuffle=False)\n",
        "\n",
        "        score = train_and_evaluate_model(train_loader, val_loader, hidden_units, dropout_rate,learning_rate, l2_lambda, optimizer_choice, num_epochs)\n",
        "        fold_scores.append(score)\n",
        "\n",
        "    avg_score = np.mean(fold_scores)\n",
        "    return avg_score\n",
        "\n",
        "# Perform Bayesian optimization\n",
        "trials = Trials()\n",
        "best = fmin(\n",
        "    fn=objective,\n",
        "    space=search_space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=50,  # Adjust the number of evaluations as needed\n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = {\n",
        "    'hidden_units': hidden_units_options[int(best['hidden_units'])],\n",
        "    'learning_rate': learning_rate_options[int(best['learning_rate'])],\n",
        "    'dropout_rate': dropout_rate_options[int(best['dropout_rate'])],\n",
        "    'l2_lambda': l2_lambda_options[int(best['l2_lambda'])],\n",
        "    'optimizer_choice': optimizer_options[int(best['optimizer_choice'])]\n",
        "}\n",
        "\n",
        "print(\"Best hyperparameters:\", best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb07b5be",
      "metadata": {},
      "source": [
        "Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b31d172",
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "best_score = float('inf')\n",
        "best_params = None\n",
        "\n",
        "# Start the hyperparameter tuning process\n",
        "for hidden_units in hyperparameters['hidden_units']:\n",
        "    for dropout_rate in hyperparameters['dropout_rate']:\n",
        "        for learning_rate in hyperparameters['learning_rate']:\n",
        "\n",
        "            fold_scores = []\n",
        "\n",
        "            kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "            for fold, (train_ids, val_ids) in enumerate(kfold.split(X_tensor)):\n",
        "                train_subsampler = Subset(dataset, train_ids)\n",
        "                val_subsampler = Subset(dataset, val_ids)\n",
        "\n",
        "                train_loader = DataLoader(train_subsampler, batch_size=32, shuffle=True)\n",
        "                val_loader = DataLoader(val_subsampler, batch_size=32, shuffle=False)\n",
        "\n",
        "                # Pass num_epochs to the function\n",
        "                score = train_and_evaluate_model(train_loader, val_loader, hidden_units, dropout_rate, learning_rate, num_epochs)\n",
        "                fold_scores.append(score)\n",
        "\n",
        "            avg_score = np.mean(fold_scores)\n",
        "            print(f\"Hidden Units: {hidden_units}, Dropout: {dropout_rate}, LR: {learning_rate}, Avg Score: {avg_score}\")\n",
        "\n",
        "            if avg_score < best_score:\n",
        "                best_score = avg_score\n",
        "                best_params = {'hidden_units': hidden_units, 'dropout_rate': dropout_rate, 'learning_rate': learning_rate}\n",
        "\n",
        "print(\"Best Score:\", best_score)\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OwDW9ovxmKmR",
      "metadata": {
        "id": "OwDW9ovxmKmR"
      },
      "source": [
        "building the model and test on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eBNJMYpbnef",
      "metadata": {
        "id": "4eBNJMYpbnef"
      },
      "outputs": [],
      "source": [
        "input_dim = X_tensor.shape[1]\n",
        "\n",
        "best_hidden_units = best_params.get('hidden_units')\n",
        "best_learning_rate = best_params.get('learning_rate')\n",
        "best_dropout_rate = best_params.get('dropout_rate')\n",
        "best_l2_lambda = best_params.get('l2_lambda')\n",
        "best_optimizer = best_params.get('optimizer_choice')\n",
        "\n",
        "model_mlp1 = MLPModel1(input_dim=input_dim, hidden_units=best_hidden_units, dropout_rate=best_dropout_rate).to(device)\n",
        "model_mlp1 = model_mlp1.to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def bestOptimizer(x, learning_rate, l2_lambda):\n",
        "    if x == 'adam':\n",
        "        return optim.Adam(model_lstm.parameters(), lr=learning_rate, weight_decay=l2_lambda)  # weight_decay is L2 regularization\n",
        "    elif x == 'rmsprop':\n",
        "        return optim.RMSprop(model_lstm.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "    elif x == 'sgd':\n",
        "        return optim.SGD(model_lstm.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "    elif x == 'adagrad':\n",
        "        return optim.Adagrad(model_lstm.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "\n",
        "optimizer = bestOptimizer(best_optimizer, best_learning_rate, best_l2_lambda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EJDpFqQAcM3k",
      "metadata": {
        "id": "EJDpFqQAcM3k"
      },
      "outputs": [],
      "source": [
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "test_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "358da1e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "display(len(X_val_tensor))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ukRyHQMucdjS",
      "metadata": {
        "id": "ukRyHQMucdjS"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "num_epochs = 50\n",
        "\n",
        "model_mlp1.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model_mlp1(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "model_mlp1.eval()  # Set the model to evaluation mode\n",
        "predictions_mlp = []\n",
        "actuals_mlp = []\n",
        "\n",
        "test_loss = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model_mlp1(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Assuming your output and targets are in the expected shape; otherwise, adjust accordingly\n",
        "        predictions_mlp.extend(outputs.cpu().numpy())\n",
        "        actuals_mlp.extend(targets.cpu().numpy())\n",
        "\n",
        "# Flatten lists if outputs are multi-dimensional\n",
        "predictions_mlp = np.array(predictions_mlp).flatten()\n",
        "actuals_mlp = np.array(actuals_mlp).flatten()\n",
        "\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "print(f'Average Test Loss: {avg_test_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9pPxc3koRA0",
      "metadata": {
        "id": "a9pPxc3koRA0"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame with predictions and actuals\n",
        "df_mlp = pd.DataFrame({'Predictions': predictions_mlp, 'Actuals': actuals_mlp})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_mlp.to_csv(\"predictions_vs_actuals_mlp.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tnaopmyYcwpk",
      "metadata": {
        "id": "tnaopmyYcwpk"
      },
      "source": [
        "meta model - MLP(we didn't tune it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AKvklUe-cwQd",
      "metadata": {
        "id": "AKvklUe-cwQd"
      },
      "outputs": [],
      "source": [
        "input_mlp = pd.read_csv(r\"predictions_vs_actuals_mlp.csv\")\n",
        "input_lstm = pd.read_csv(r\"predictions_vs_actuals_lstm.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cPsYj0AuplLA",
      "metadata": {
        "id": "cPsYj0AuplLA"
      },
      "outputs": [],
      "source": [
        "print(input_mlp.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gbd3VgoqprAi",
      "metadata": {
        "id": "gbd3VgoqprAi"
      },
      "outputs": [],
      "source": [
        "print(input_lstm.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SCrO4K8-pr80",
      "metadata": {
        "id": "SCrO4K8-pr80"
      },
      "outputs": [],
      "source": [
        "combined_df = input_mlp.join(input_lstm, lsuffix='_mlp', rsuffix='_lstm')\n",
        "\n",
        "print(combined_df.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RaH9ePdl3GvY",
      "metadata": {
        "id": "RaH9ePdl3GvY"
      },
      "outputs": [],
      "source": [
        "X_new_train = combined_df[['Predictions_mlp', 'Predictions_lstm']].values\n",
        "y_new_train = combined_df['Actuals_lstm'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-QwFJgHh4L5S",
      "metadata": {
        "id": "-QwFJgHh4L5S"
      },
      "outputs": [],
      "source": [
        "class MetaMLPModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size=1, dropout_rate=0.2):\n",
        "        super(MetaMLPModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 32)\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        self.fc3 = nn.Linear(64, 128)\n",
        "        self.fc4 = nn.Linear(128, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc4(x)  # No dropout before the output layer\n",
        "        return x\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "input_size = 2\n",
        "meta_model = MetaMLPModel(input_size=input_size).to(device)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_new_train_tensor = torch.tensor(X_new_train, dtype=torch.float).to(device)\n",
        "y_new_train_tensor = torch.tensor(y_new_train, dtype=torch.float).view(-1, 1).to(device)\n",
        "\n",
        "# Create TensorDataset and DataLoader\n",
        "meta_train_dataset = TensorDataset(X_new_train_tensor, y_new_train_tensor)\n",
        "meta_train_loader = DataLoader(dataset=meta_train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(meta_model.parameters(), lr=0.05, weight_decay= 0.04)  # Set a learning rate\n",
        "\n",
        "# Training loop\n",
        "meta_model.train()\n",
        "epochs = 200  # Set the number of epochs\n",
        "for epoch in range(epochs):\n",
        "    for inputs, targets in meta_train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = meta_model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Optional: Print the loss every epoch\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zTZIZE-35m6H",
      "metadata": {
        "id": "zTZIZE-35m6H"
      },
      "source": [
        "Testing set accuracy for our stacking model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f94iv7eO5jtH",
      "metadata": {
        "id": "f94iv7eO5jtH"
      },
      "outputs": [],
      "source": [
        "X_test_lstm = np.array(X_test)\n",
        "y_test_lstm = np.array(y_test).reshape(-1, 1)\n",
        "\n",
        "X_test_lstm = X_test_lstm.reshape((X_test_lstm.shape[0], 1, X_test_lstm.shape[1]))\n",
        "\n",
        "test_dataset = TensorDataset(torch.Tensor(X_test_lstm), torch.Tensor(y_test_lstm))\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Lv2YCKVD6j2D",
      "metadata": {
        "id": "Lv2YCKVD6j2D"
      },
      "outputs": [],
      "source": [
        "model_lstm.eval()\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "test_loss = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model_lstm(inputs)\n",
        "\n",
        "        # Move predictions and actuals to CPU and convert them to numpy arrays\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(targets.cpu().numpy())\n",
        "        loss = criterion(outputs, targets)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "predictions = np.array(predictions).flatten()  # Flattening in case the outputs are multi-dimensional\n",
        "actuals = np.array(actuals).flatten()\n",
        "\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "print(f'Average Test Loss: {avg_test_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qUg4l9ua6lq4",
      "metadata": {
        "id": "qUg4l9ua6lq4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({'Predictions': predictions, 'Actuals': actuals})\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(\"intermediate_lstm.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d45iR3516uwC",
      "metadata": {
        "id": "d45iR3516uwC"
      },
      "outputs": [],
      "source": [
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exfYx51i7F_Q",
      "metadata": {
        "id": "exfYx51i7F_Q"
      },
      "outputs": [],
      "source": [
        "model_mlp1.eval()  # Set the model to evaluation mode\n",
        "predictions_mlp = []\n",
        "actuals_mlp = []\n",
        "\n",
        "test_loss = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model_mlp1(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Assuming your output and targets are in the expected shape; otherwise, adjust accordingly\n",
        "        predictions_mlp.extend(outputs.cpu().numpy())\n",
        "        actuals_mlp.extend(targets.cpu().numpy())\n",
        "\n",
        "# Flatten lists if outputs are multi-dimensional\n",
        "predictions_mlp = np.array(predictions_mlp).flatten()\n",
        "actuals_mlp = np.array(actuals_mlp).flatten()\n",
        "\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "print(f'Average Test Loss: {avg_test_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U-Ou8H9Y7QDV",
      "metadata": {
        "id": "U-Ou8H9Y7QDV"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame with predictions and actuals\n",
        "df_mlp = pd.DataFrame({'Predictions': predictions_mlp, 'Actuals': actuals_mlp})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_mlp.to_csv(\"intermediate_mlp1.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3yduXJOt7YE9",
      "metadata": {
        "id": "3yduXJOt7YE9"
      },
      "outputs": [],
      "source": [
        "testinput_mlp1 = pd.read_csv(r\"intermediate_mlp1.csv\")\n",
        "testinput_lstm = pd.read_csv(r\"intermediate_lstm.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M3gXOBeq7qoz",
      "metadata": {
        "id": "M3gXOBeq7qoz"
      },
      "outputs": [],
      "source": [
        "combined_df_test = testinput_mlp1.join(testinput_lstm, lsuffix='_mlp', rsuffix='_lstm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8Qy1JqDx76DA",
      "metadata": {
        "id": "8Qy1JqDx76DA"
      },
      "outputs": [],
      "source": [
        "X_new_test = combined_df_test[['Predictions_mlp', 'Predictions_lstm']].values\n",
        "y_new_test = combined_df_test['Actuals_lstm'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GnyXJXRb8CHa",
      "metadata": {
        "id": "GnyXJXRb8CHa"
      },
      "outputs": [],
      "source": [
        "X_new_test = torch.tensor(X_new_test, dtype=torch.float).to(device)\n",
        "y_new_test = torch.tensor(y_new_test, dtype=torch.float).view(-1, 1).to(device)\n",
        "\n",
        "meta_test_dataset = TensorDataset(X_new_test, y_new_test)\n",
        "meta_test_loader = DataLoader(dataset=meta_test_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Yp4vt4Ij8QiI",
      "metadata": {
        "id": "Yp4vt4Ij8QiI"
      },
      "outputs": [],
      "source": [
        "meta_model.eval()\n",
        "test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in meta_test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = meta_model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "avg_test_loss = test_loss / len(meta_test_loader)\n",
        "print(f'Average Test Loss: {avg_test_loss}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
